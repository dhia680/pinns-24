{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "tU5I-fa2TwlA",
        "ockWbzZWTD_T",
        "1HzLm_aNTNNG",
        "Ho44u8OpTZeY",
        "Ha1l-kLpSGnt",
        "IlmArz_0_LhI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Helmholtz PDE example : cas (d)**"
      ],
      "metadata": {
        "id": "HxU5urQpj16P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Discussion sur les formulations*\n",
        "Type guide d'onde avec : $\\Omega = [-L, L] \\times [-h, h]$\n",
        "\n",
        "Formulation sur le site du projet :\n",
        "\\begin{align*}\n",
        "& -\\Delta \\varphi(x) - \\omega^2 \\, \\varphi(x) = f(x) \\quad \\text{in} \\enspace \\Omega\\\\\n",
        "& \\varphi = 0 \\quad \\text{on} \\enspace \\{ x = \\pm h \\} \\cup \\{ y = -L \\} := \\partial \\Omega\\\\\n",
        "& \\partial_n \\varphi - ik \\varphi = 0 \\quad \\text{on} \\enspace \\partial \\Omega\n",
        "\\end{align*}\n",
        "\n",
        "Condition de Robin: Cette condition est plus générale que la condition de Neumann et permet de modéliser des pertes d'énergie à la frontière. <br>\n",
        "Le paramètre k contrôle le degré de perte:<br>\n",
        "- k=0 correspond à la condition de Neumann pure (pas de perte).<br>\n",
        "- k>0 correspond à des pertes d'énergie (absorption).<br>\n",
        "- k<0 correspond à un gain d'énergie (amplification).<br>\n",
        "\n",
        "---> Pas sûr que cette formulation soit bonne. Dans la dernière condition $\\varphi$ est nulle.\n",
        "<!-- Exemples de sources:  -->\n",
        "<!-- - 1) Considérons une fonction source de la forme $f(x,y)=sin(kx)sin(ly)$. <br>\n",
        "    ==> (A VERIFIER ! (gemini) ) En utilisant la méthode de séparation des variables, on trouve:\n",
        "    $φ(x,y)=∑\\_n∑\\_m ​A_{mn}.cos(kx)cos(ly).e^{−β_{mn}z} $\n",
        "    où\n",
        "    $\\partial_n \\varphi - ik\\varphi = 0$ on  $\\partial \\Omega$ ==> $\\frac{l}{h} = tan(kh)$ <br>\n",
        "\n",
        "    TO BE CORRECTED -->\n",
        "\n",
        "<br>\n",
        "\n",
        "####Formulation alternative mais **pas sûrement bonne** (avec une Cste non nulle pour éviter la sol nulle) : <br>\n",
        "$\\Omega = [-L, L] \\times [-h, h]$\n",
        "\\begin{align*}\n",
        "& \\Delta \\varphi(x,y) + k^2 \\, \\varphi(x,y) = 0 \\quad \\text{in} \\enspace \\Omega \\\\\n",
        "& \\varphi = Cste \\quad \\text{on} \\enspace \\{ x = \\pm L \\} \\\\\n",
        "& \\partial_n \\varphi + \\alpha . \\varphi = 0 \\quad \\text{on} \\enspace \\{ y = \\pm h \\}\n",
        "\\end{align*}\n",
        "<br>"
      ],
      "metadata": {
        "id": "WbQWF7rQvCc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##*Formulation retenue pour le code suivant (source cste non nulle)*:\n",
        "\n",
        "\\begin{align*}\n",
        "& -\\Delta \\varphi(x) - \\omega^2 \\, \\varphi(x) = 1 \\quad \\text{in} \\enspace \\Omega\\\\\n",
        "& \\varphi = 0 \\quad \\text{on} \\enspace \\{ x = \\pm h \\} \\cup \\{ y = -L \\} := \\partial \\Omega\\\\\n",
        "& \\partial_n \\varphi = 0 \\quad \\text{on} \\enspace \\partial \\Omega\n",
        "\\end{align*}\n",
        "\n",
        "(à tenter : source cste complexe non réelle.. ensuite source variable)"
      ],
      "metadata": {
        "id": "vl68Kl06jmTH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##General imports"
      ],
      "metadata": {
        "id": "tU5I-fa2TwlA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "J3VfVMA8uOF7"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "from collections import namedtuple\n",
        "from abc import ABC, abstractmethod\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import lr_scheduler\n",
        "import matplotlib.pyplot as plt\n",
        "import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ma tentative"
      ],
      "metadata": {
        "id": "pGbAjB38TAiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Mesh & Collocation points"
      ],
      "metadata": {
        "id": "ockWbzZWTD_T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def generate_uniform_mesh(L, h, Nx, Ny):\n",
        "    x_points = np.linspace(-L, L, Nx)\n",
        "    y_points = np.linspace(-h, h, Ny)\n",
        "    mesh_points = np.array(np.meshgrid(x_points, y_points)).T.reshape(-1, 2)\n",
        "    return mesh_points\n",
        "\n",
        "# Example parameters\n",
        "L = 1.0\n",
        "h = 0.5\n",
        "Nx = 30\n",
        "Ny = 15\n",
        "\n",
        "# Generate uniform mesh\n",
        "collocation_points = generate_uniform_mesh(L, h, Nx, Ny)\n",
        "# print(\"Collocation points:\", collocation_points)\n",
        "print(\"shape of the mesh:\", collocation_points.shape)\n",
        "\n",
        "# Calculate the number of colocation points\n",
        "number_of_colocation_points = collocation_points.shape[0]\n",
        "print(\"Number of colocation points:\", number_of_colocation_points)\n",
        "\n",
        "import torch\n",
        "# Extract x and y coordinates from collocation_points\n",
        "x_points = collocation_points[:, 0]\n",
        "y_points = collocation_points[:, 1]\n",
        "\n",
        "# Determine the boundary indices\n",
        "boundary_indices = torch.arange(number_of_colocation_points)[\n",
        "    (x_points == -L) | (x_points == L) | (y_points == -h) | (y_points == h)\n",
        "]\n",
        "\n",
        "print(\"Boundary indices:\", boundary_indices)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h-SHC87k6MKN",
        "outputId": "44ad4413-b511-4a43-b02d-354310e08dea"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of the mesh: (450, 2)\n",
            "Number of colocation points: 450\n",
            "Boundary indices: tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
            "         14,  15,  29,  30,  44,  45,  59,  60,  74,  75,  89,  90, 104, 105,\n",
            "        119, 120, 134, 135, 149, 150, 164, 165, 179, 180, 194, 195, 209, 210,\n",
            "        224, 225, 239, 240, 254, 255, 269, 270, 284, 285, 299, 300, 314, 315,\n",
            "        329, 330, 344, 345, 359, 360, 374, 375, 389, 390, 404, 405, 419, 420,\n",
            "        434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447,\n",
            "        448, 449])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###NN Architechture + Loss computation"
      ],
      "metadata": {
        "id": "1HzLm_aNTNNG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.autograd import grad\n",
        "\n",
        "# Define the MLP architecture\n",
        "class ComplexMLP(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(ComplexMLP, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)"
      ],
      "metadata": {
        "id": "j3Agw6hmNmLc"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the loss function incorporating the Helmholtz equation and boundary conditions\n",
        "\n",
        "def physics_loss(model, x, true_boundary_data, boundary_indices):\n",
        "    # Compute predictions for all points\n",
        "    predictions = model(x)\n",
        "    predictions.requires_grad = True\n",
        "\n",
        "    # Extract real and imaginary parts pf predictions\n",
        "    u_real, u_imag = torch.chunk(predictions, 2, dim=1)\n",
        "\n",
        "    # Compute derivatives\n",
        "    grad_u_real = grad(u_real, x, grad_outputs=torch.ones_like(u_real), create_graph=True)[0]\n",
        "    grad_u_imag = grad(u_imag, x, grad_outputs=torch.ones_like(u_imag), create_graph=True)[0]\n",
        "    laplacian_u_real = torch.sum(grad(grad_u_real, x, grad_outputs=torch.ones_like(grad_u_real), create_graph=True)[0], dim=1)\n",
        "    laplacian_u_imag = torch.sum(grad(grad_u_imag, x, grad_outputs=torch.ones_like(grad_u_imag), create_graph=True)[0], dim=1)\n",
        "\n",
        "    # Calculate the residual of the Helmholtz equation\n",
        "    residual = laplacian_u_real + laplacian_u_imag + k_squared * (u_real + u_imag) + 1    # because I have assume a constant source function f = 1\n",
        "\n",
        "\n",
        "    # Compute the normal derivatives at boundary points\n",
        "    dudx = grad(u_real, x, grad_outputs=torch.ones_like(u_real), create_graph=True)[0][:, 0]\n",
        "    dudy = grad(u_real, x, grad_outputs=torch.ones_like(u_real), create_graph=True)[0][:, 1]\n",
        "    dvdx = grad(u_imag, x, grad_outputs=torch.ones_like(u_imag), create_graph=True)[0][:, 0]\n",
        "    dvdy = grad(u_imag, x, grad_outputs=torch.ones_like(u_imag), create_graph=True)[0][:, 1]\n",
        "    # grad_u_mag = torch.sqrt(grad_u_real**2 + grad_u_imag**2)  # Magnitude of the whole gradient (not only the normal derivative)\n",
        "\n",
        "    # Compute normal derivatives based on boundary direction\n",
        "    normal_derivatives_x = (dudx * (x[:, 0] == -L) - dudx * (x[:, 0] == L)) + (dvdx * (x[:, 0] == -L) - dvdx * (x[:, 0] == L))\n",
        "    normal_derivatives_y = (dudy * (x[:, 1] == -h) - dudy * (x[:, 1] == h)) + (dvdy * (x[:, 1] == -h) - dvdy * (x[:, 1] == h))\n",
        "    normal_derivatives = torch.sqrt(normal_derivatives_x**2 + normal_derivatives_y**2)  # Magnitude of the normal derivatives\n",
        "\n",
        "\n",
        "    # Extract predictions for boundary points\n",
        "    boundary_predictions = predictions[boundary_indices, :]\n",
        "\n",
        "    # Enforce boundary conditions\n",
        "    boundary_loss_1 = torch.mean((boundary_predictions - true_boundary_data)**2)       # Mean squared error\n",
        "    boundary_loss_2 = torch.mean(normal_derivatives)                                   # Penalty on normal derivative\n",
        "    # boundary_loss_2 = torch.mean(grad_u_mag[boundary_indices]**2)                    # si jamais on veut que la dérivée de u soit nulle et pas seulement la dérivé normale\n",
        "\n",
        "    boundary_loss = boundary_loss_1 + boundary_loss_2                                  #TO check\n",
        "\n",
        "\n",
        "    # Total loss\n",
        "    total_loss = torch.mean(torch.square(residual)) + boundary_loss                   # Loss_residuelle + loss_boundary_1 + loss_boundary_2   (JE DOIS AJOUTER DES COEFFS une fois que ça marche)\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "IaJMoKyl39T0"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Predicting the solution (still needs some debugging of required grads...)"
      ],
      "metadata": {
        "id": "Ho44u8OpTZeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the data to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define the model\n",
        "input_dim = 2*number_of_colocation_points     #according to the mesh of the set [-L,L].[-h,h] (the number of points) !\n",
        "hidden_dim = 500  # to adapt\n",
        "output_dim = input_dim    #the model returns a 2D_tensor of two vectors : u_real and u_imag predictions on every colocation point\n",
        "# Define other parameters\n",
        "\n",
        "k_squared = 1e6  # Wave number squared in Helmholtz equation (O.G de k --> 10^3 en acoustique VS 10^6 en éléctromagnétique)\n",
        "model = ComplexMLP(input_dim, hidden_dim, output_dim).to(device)\n",
        "\n",
        "\n",
        "\n",
        "# Generate true boundary data\n",
        "true_boundary_data = torch.zeros((len(boundary_indices), 2), device=device)   # assuming null Dirichlet boundary conditions for both u and derivée_normale(u)\n",
        "# Reshape collocation_points to match the input shape expected by the model\n",
        "collocation_points = torch.tensor(collocation_points, dtype=torch.float32, device=device)\n",
        "# collocation_points = collocation_points.unsqueeze(0)  # Add a batch dimension\n",
        "# collocation_points = collocation_points.expand(1, number_of_colocation_points, 2).reshape(-1, input_dim)\n",
        "collocation_points = collocation_points.unsqueeze(0)  # Add a batch dimension\n",
        "collocation_points = collocation_points.expand(-1, number_of_colocation_points, -1).reshape(-1, input_dim)\n",
        "\n",
        "boundary_indices = boundary_indices.to(device)"
      ],
      "metadata": {
        "id": "mgwE74twNwlf"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.002)\n",
        "losses = []\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    loss = physics_loss(model, collocation_points, true_boundary_data, boundary_indices)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses.append(loss.item())  # Append the current loss to the list\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "-yEZUJT3iq5K",
        "outputId": "9fc45832-baca-47f6-f3df-6a29d73b8eae"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "you can only change requires_grad flags of leaf variables.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-83-f676597e007a>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mphysics_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollocation_points\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_boundary_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboundary_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-81-d9a35a74c554>\u001b[0m in \u001b[0;36mphysics_loss\u001b[0;34m(model, x, true_boundary_data, boundary_indices)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# Compute predictions for all points\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Extract real and imaginary parts pf predictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: you can only change requires_grad flags of leaf variables."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Plotting the solution (not yet tested)"
      ],
      "metadata": {
        "id": "Ha1l-kLpSGnt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "x_points = collocation_points[:, 0]\n",
        "y_points = collocation_points[:, 1]\n",
        "\n",
        "# Evaluate the model on the collocation points\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    predictions = model(torch.tensor(collocation_points, dtype=torch.float32))\n",
        "\n",
        "# Compute the magnitude of the complex-valued function U\n",
        "U_real, U_imag = torch.chunk(predictions, 2, dim=1)\n",
        "U_magnitude = torch.sqrt(U_real**2 + U_imag**2).numpy()\n",
        "\n",
        "# Reshape the magnitude to match the shape of the collocation points\n",
        "U_magnitude_plot = U_magnitude.reshape((-1, Ny, Nx))"
      ],
      "metadata": {
        "id": "Da5sY9aESS82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the 3D surface plot\n",
        "fig = plt.figure()\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "X, Y = np.meshgrid(x_points, y_points)\n",
        "ax.plot_surface(X, Y, U_magnitude_plot[0], cmap='viridis')\n",
        "ax.set_xlabel('X')\n",
        "ax.set_ylabel('Y')\n",
        "ax.set_zlabel('|U|')\n",
        "ax.set_title('Magnitude of the Complex-Valued Predicted Function U (3D Surface)')\n",
        "plt.show()\n",
        "\n",
        "# Plot the heat map\n",
        "plt.figure()\n",
        "plt.imshow(U_magnitude_plot[0], extent=[-L, L, -h, h], origin='lower', cmap='viridis')\n",
        "plt.colorbar(label='|U|')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.title('Magnitude of the Complex-Valued Predicted Function U (Heat Map)')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "YNrQ93YJSGHn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "L'article https://core.ac.uk/download/pdf/82014892.pdf traite le cas (d) en 1D avec des éléments finis (pourrait être intéressant)"
      ],
      "metadata": {
        "id": "NV2vS607CC5a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eventuelle exploitation du code d'Antoine (Class Helmholtz_d)... A finir"
      ],
      "metadata": {
        "id": "IlmArz_0_LhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save_gif_pil(outfile, files, fps=5, loop=0):\n",
        "    \"\"\"Helper function for saving GIFs\"\"\"\n",
        "    imgs = [Image.open(file) for file in files]\n",
        "    imgs[0].save(fp=outfile, format='GIF', append_images=imgs[1:],\n",
        "                 save_all=True, duration=int(1000 / fps), loop=loop)"
      ],
      "metadata": {
        "id": "JoLwZZvoTrwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CauchyProblem(ABC):\n",
        "    @abstractmethod\n",
        "    def reference_solution(self, x):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def residual(self, ddy, dy, y):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def cauchy_conditions(self, dy, y):\n",
        "        pass\n",
        "\n",
        "\n",
        "class Oscillator(CauchyProblem):\n",
        "    \"\"\"\n",
        "    Equation\n",
        "        m y''(x) + mu y'(x) + k y(x) = 0,  x in (0, 1)\n",
        "    Initial condition\n",
        "        y(0) = 1\n",
        "        y'(0) = 1\n",
        "    Here\n",
        "        - dumping = mu / 2m\n",
        "        - pulsation = sqrt(k/m)\n",
        "    \"\"\"\n",
        "    def __init__(self, dumping, pulsation):\n",
        "        assert dumping < pulsation\n",
        "        self.d = dumping\n",
        "        self.w0 = pulsation\n",
        "        self.y0 = 1.0\n",
        "        self.dy0 = 0.0\n",
        "\n",
        "    def reference_solution(self, x):\n",
        "        \"\"\"Defines the analytical solution to the 1D underdamped harmonic\n",
        "        oscillator problem.\n",
        "        Equations taken from: https://beltoforion.de/en/harmonic_oscillator/\n",
        "        \"\"\"\n",
        "        w = np.sqrt(self.w0 ** 2 - self.d ** 2)\n",
        "        phi = np.arctan(-self.d / w)\n",
        "        a = 1 / (2 * np.cos(phi))\n",
        "        cos = torch.cos(phi + w * x)\n",
        "        # sin = torch.sin(phi + w * x)\n",
        "        exp = torch.exp(-self.d * x)\n",
        "        return exp * 2 * a * cos\n",
        "\n",
        "    def residual(self, ddy, dy, y):\n",
        "        mu = 2 * self.d\n",
        "        k = self.w0 ** 2\n",
        "        return ddy + mu * dy + k * y\n",
        "\n",
        "    def cauchy_conditions(self, dy, y):\n",
        "        return namedtuple(\"CauchyCond\", [\"y0\", \"dy0\"])(y[0] - self.y0,\n",
        "                                                       dy[0] - self.dy0)\n",
        "\n",
        "\n",
        "class Helmholtz_d(CauchyProblem):\n",
        "    \"\"\"\n",
        "    Equation\n",
        "    Helmhotlz type guide d'onde (cas d)\n",
        "    \"\"\"\n",
        "    def __init__(self, pulsation, L, h):\n",
        "        self.w0 = pulsation\n",
        "        self.L = L\n",
        "        self.h = h\n",
        "        self.y0 = 0.0\n",
        "\n",
        "\n",
        "    def residual(self, ddy, dy, y):\n",
        "        mu = 2 * self.d\n",
        "        k = self.w0 ** 2\n",
        "        return ddy + mu * dy + k * y\n",
        "\n",
        "    def cauchy_conditions(self, dy, y):\n",
        "        return namedtuple(\"CauchyCond\", [\"y0\", \"dy0\"])(y[0] - self.y0,\n",
        "                                                       dy[0] - self.dy0)"
      ],
      "metadata": {
        "id": "Zwc6C05DuXrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SinActivation(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sin(x)\n",
        "\n",
        "\n",
        "class FullyConnectedNetwork(nn.Module):\n",
        "    \"\"\"Defines a connected network\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, hidden_dim, nb_layers):\n",
        "        super().__init__()\n",
        "        activation = nn.Tanh\n",
        "        # activations = [nn.Tanh for _ in range(nb_layers - 1)]\n",
        "        activations = [SinActivation for _ in range(nb_layers - 1)]\n",
        "        activations[-1] = SinActivation\n",
        "        self.fcs = nn.Sequential(nn.Linear(input_dim, hidden_dim), activation())\n",
        "        self.fch = nn.Sequential(\n",
        "            *[nn.Sequential(nn.Linear(hidden_dim, hidden_dim), act())\n",
        "              for act in activations])\n",
        "        self.fce = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fcs(x)\n",
        "        x = self.fch(x)\n",
        "        x = self.fce(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "KRlmtH_duXup"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_result(x_exact, y_exact, xp, yh, epoch, file_name):\n",
        "    \"\"\"Pretty plot training results\"\"\"\n",
        "    plt.figure(figsize=(8, 4))\n",
        "    plt.plot(x_exact, y_exact, color=\"grey\", linewidth=2,\n",
        "             alpha=0.8,\n",
        "             label=\"Exact solution\")\n",
        "    plt.plot(x_exact, yh, color=\"tab:blue\", linewidth=4, alpha=0.8,\n",
        "             label=\"Neural network prediction\")\n",
        "    plt.scatter(xp, -0 * torch.ones_like(xp), s=60, color=\"tab:green\",\n",
        "                alpha=0.4,\n",
        "                label='Physics loss training locations')\n",
        "    legend = plt.legend(loc=(1.01, 0.34), frameon=False, fontsize=\"large\")\n",
        "    plt.setp(legend.get_texts(), color=\"k\")\n",
        "    plt.xlim(-0.05, 1.05)\n",
        "    plt.ylim(-1.1, 1.1)\n",
        "    plt.text(1.065, 0.7, \"Training step: %i\" % (epoch + 1), fontsize=\"xx-large\",\n",
        "             color=\"k\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.savefig(file_name, bbox_inches='tight',\n",
        "                pad_inches=0.1, dpi=100, facecolor=\"white\")\n",
        "\n",
        "\n",
        "def plot_losses(losses):\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.subplots(2, 2)\n",
        "    epoch_list = range(len(losses.residual))\n",
        "    ax[0][0].semilogy(epoch_list, losses.total,\n",
        "                      color=\"black\",\n",
        "                      label=\"Total loss\")\n",
        "    ax[0][1].semilogy(epoch_list, losses.residual,\n",
        "                      color=\"red\",\n",
        "                      label=\"loss residual\")\n",
        "    ax[1][0].semilogy(epoch_list, losses.init_value,\n",
        "                      color=\"green\",\n",
        "                      label=\"loss (y[0] - 1)**2\")\n",
        "    ax[1][1].semilogy(epoch_list, losses.init_derivative,\n",
        "                      color=\"green\",\n",
        "                      label=\"loss dy/dx[0]**2\")\n",
        "    [(a.grid(), a.legend()) for aa in ax for a in aa]\n",
        "    fig.savefig(\"loss_convergence\")"
      ],
      "metadata": {
        "id": "1dsR0hjXuXx7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SavedLosses:\n",
        "    def __init__(self):\n",
        "        self.total = []\n",
        "        self.residual = []\n",
        "        self.init_value = []\n",
        "        self.init_derivative = []\n",
        "\n",
        "    def save(self, total, residual, init_value, init_derivative):\n",
        "        self.total.append(total)\n",
        "        self.residual.append(residual)\n",
        "        self.init_value.append(init_value)\n",
        "        self.init_derivative.append(init_derivative)"
      ],
      "metadata": {
        "id": "RI7c5wPCuw18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_pinns(cauchy_problem: CauchyProblem):\n",
        "    plots_dir = \"./plots\"\n",
        "    if not os.path.exists(plots_dir):\n",
        "        os.makedirs(plots_dir)\n",
        "    # get the analytical solution over the full domain\n",
        "    n_pts = 500\n",
        "    x_exact = torch.linspace(0, 1, n_pts).view(-1, 1).requires_grad_(True)\n",
        "    y_exact = cauchy_problem.reference_solution(x_exact).view(-1, 1)\n",
        "\n",
        "    # sample locations over the problem domain\n",
        "    n_col_pts = 30\n",
        "    collocation_pts = torch.linspace(0, 1,\n",
        "                                     n_col_pts,\n",
        "                                     requires_grad=True).view(-1, 1)\n",
        "\n",
        "    model = FullyConnectedNetwork(input_dim=1,\n",
        "                                  output_dim=1,\n",
        "                                  hidden_dim=32,\n",
        "                                  nb_layers=3)\n",
        "\n",
        "    coef_loss_residual = 5*1e-5   #Original value = 1e-5\n",
        "    coef_loss_y0 = 1e1\n",
        "    coef_loss_dy0 = 1e-4\n",
        "\n",
        "    learning_rate = 1e-4\n",
        "    optimizer = torch.optim.Adam(model.parameters(),\n",
        "                                 lr=learning_rate)\n",
        "\n",
        "    n_epoch = 28_000\n",
        "\n",
        "    def lr_multiplier(epoch):\n",
        "        return 1 ** (epoch / n_epoch)\n",
        "\n",
        "    scheduler = lr_scheduler.LambdaLR(optimizer,\n",
        "                                      lr_lambda=lr_multiplier)\n",
        "    files = []\n",
        "    losses = SavedLosses()\n",
        "    for epoch in tqdm.tqdm(range(n_epoch)):\n",
        "        optimizer.zero_grad()\n",
        "        y = model(collocation_pts)\n",
        "        # computes dy/dx\n",
        "        dy = torch.autograd.grad(y, collocation_pts,\n",
        "                                 torch.ones_like(y),\n",
        "                                 create_graph=True)[0]\n",
        "\n",
        "        # computes d^2y/dx^2\n",
        "        ddy = torch.autograd.grad(dy, collocation_pts,\n",
        "                                  torch.ones_like(dy),\n",
        "                                  create_graph=True)[0]\n",
        "\n",
        "        # computes the residual of the 1D harmonic oscillator differential\n",
        "        residual = cauchy_problem.residual(ddy, dy, y)\n",
        "        loss_residual = torch.mean(residual ** 2)\n",
        "        cauchy_cond = cauchy_problem.cauchy_conditions(dy, y)\n",
        "        loss_y0, loss_dy0 = cauchy_cond.y0 ** 2, cauchy_cond.dy0 ** 2\n",
        "\n",
        "        # backpropagate joint loss\n",
        "        loss = (coef_loss_residual * loss_residual\n",
        "                + coef_loss_y0 * loss_y0\n",
        "                + coef_loss_dy0 * loss_dy0)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "\n",
        "        losses.save(total=loss.item(),\n",
        "                    residual=loss_residual.item(),\n",
        "                    init_value=loss_y0.item(),\n",
        "                    init_derivative=loss_dy0.item())\n",
        "\n",
        "        # plot the result as training progresses\n",
        "        if (epoch + 1) % 500 == 0:\n",
        "            print(\"\")\n",
        "            print(f\"* Learning rate = {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "            print(f\"* Losses\")\n",
        "            print(f\"    - Residual loss     = \"\n",
        "                  f\"{loss_residual.item():.2e}\")\n",
        "            print(f\"    - Initial cond loss = \"\n",
        "                  f\"{loss_y0.item():.2e}   {loss_dy0.item():.2e}\")\n",
        "            print(f\"=> Total weighted loss = {loss.item():.2e}\")\n",
        "            yh = model(x_exact).detach()\n",
        "            xp = collocation_pts.detach()\n",
        "\n",
        "            file_name = f\"{plots_dir}/pinn_{epoch + 1:8d}.png\"\n",
        "            plot_result(x_exact=x_exact.detach().numpy(),\n",
        "                        y_exact=y_exact.detach().numpy(),\n",
        "                        xp=xp, yh=yh, epoch=epoch,\n",
        "                        file_name=file_name)\n",
        "            files.append(file_name)\n",
        "\n",
        "    save_gif_pil(\"pinn.gif\", files, fps=20, loop=0)\n",
        "    plot_losses(losses)"
      ],
      "metadata": {
        "id": "Cse0yTYvuxFp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == '__main__':\n",
        "    torch.manual_seed(123)\n",
        "    oscillator = Oscillator(dumping=2, pulsation=20)\n",
        "    compute_pinn = True\n",
        "    compute_from_data = False\n",
        "    if compute_pinn:\n",
        "        train_pinns(oscillator)\n",
        "    if compute_from_data:\n",
        "        train_from_data(oscillator)\n",
        "    print(\"done!\")"
      ],
      "metadata": {
        "id": "DPiHaHz2uygZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}